# Task Worker - Sistema de Busca de Leads via Google Maps

**Status:** PLANEJAMENTO  
**Data:** 2025-08-20  
**Respons√°vel:** Equipe de Desenvolvimento  
**Prioridade:** ALTA  

---

## üìã Objetivo

Desenvolver um sistema automatizado de coleta de leads B2B atrav√©s do Google Maps, utilizando web scraping inteligente para capturar informa√ß√µes de empresas locais e enriquec√™-las com dados de contato verificados.

### Meta Principal
Criar um worker/processo que execute buscas sistem√°ticas no Google Maps por segmentos espec√≠ficos e regi√µes, coletando dados empresariais para alimentar a base de leads do LeadsRapido.

---

## üéØ Escopo do Projeto

### Funcionalidades Core
1. **Busca Automatizada:** Pesquisar empresas por categoria/segmento + localiza√ß√£o
2. **Extra√ß√£o de Dados:** Capturar informa√ß√µes p√∫blicas das empresas
3. **Enriquecimento:** Complementar dados com APIs externas
4. **Valida√ß√£o:** Verificar qualidade e veracidade dos dados
5. **Armazenamento:** Salvar leads na base de dados estruturada
6. **Monitoramento:** Acompanhar performance e detectar bloqueios

### Segmentos Priorit√°rios
- Restaurantes e alimenta√ß√£o
- Servi√ßos automotivos
- Cl√≠nicas e consult√≥rios
- Escrit√≥rios de advocacia
- Imobili√°rias
- Academias e fitness
- Sal√µes de beleza
- Lojas de varejo
- Empresas de tecnologia
- Consultorias

---

## üõ† Stack Tecnol√≥gico

### Backend Worker
- **Runtime:** Node.js 20+ TypeScript
- **Web Scraping:** Puppeteer + Playwright (fallback)
- **Queue System:** RabbitMQ + amqplib
- **Database:** PostgreSQL + Prisma ORM
- **APIs Externas:** ReceitaWS, Serasa, Google Places API
- **Proxy Management:** Bright Data ou Proxy-Cheap
- **Monitoring:** Winston + Prometheus

### Infraestrutura
- **Containers:** Docker + Docker Compose
- **Orchestration:** Kubernetes (produ√ß√£o)
- **Storage:** MinIO (S3-compatible) para screenshots/logs
- **Message Broker:** RabbitMQ para filas e rate limiting
- **Secrets:** HashiCorp Vault ou K8s Secrets

---

## üìä Arquitetura do Sistema

```mermaid
graph TD
    A[Scheduler] --> B[RabbitMQ Queues]
    B --> C[Worker Pool]
    C --> D[Browser Manager]
    D --> E[Google Maps]
    C --> F[Data Enrichment]
    F --> G[Validation]
    G --> H[Database]
    
    I[Monitoring] --> C
    J[Proxy Manager] --> D
    K[Rate Limiter] --> C
```

### Componentes Principais

#### 1. **Search Scheduler**
- Agenda buscas por regi√£o/segmento
- Evita duplica√ß√£o de esfor√ßos
- Controla frequ√™ncia de execu√ß√£o
- Prioriza segmentos com maior demanda

#### 2. **Worker Pool**
- Pool de workers paralelos
- Cada worker gerencia um browser
- Auto-scaling baseado em carga
- Restart autom√°tico em caso de falha

#### 3. **Browser Manager**
- Configura√ß√£o otimizada do Puppeteer
- Rota√ß√£o de User-Agents
- Gest√£o de cookies/sess√µes
- Screenshot para debugging

#### 4. **Data Enrichment Engine**
- Busca CNPJ na Receita Federal
- Valida√ß√£o de telefones/emails
- Scoring de qualidade dos dados
- Categoriza√ß√£o autom√°tica

#### 5. **Validation System**
- Detec√ß√£o de duplicatas
- Verifica√ß√£o de dados obrigat√≥rios
- Scoring de confiabilidade
- Filtros de qualidade

---

## üìã Tarefas Detalhadas

### FASE 1: Infraestrutura Base (Sprint 1 - 2 semanas)

#### T1.1: Setup do Ambiente de Desenvolvimento
- [ ] **T1.1.1:** Configurar projeto Node.js + TypeScript
  - Criar structure de pastas (`/src`, `/workers`, `/scripts`, `/tests`)
  - Configurar ESLint, Prettier, Husky
  - Setup do Docker Compose para desenvolvimento
  - **Estimativa:** 1 dia
  - **Respons√°vel:** Tech Lead
d
- [ ] **T1.1.2:** Configurar Banco de Dados
  - Criar schemas para leads, searches, logs
  - Implementar migrations com Prisma
  - Configurar indexes para performance
  - **Estimativa:** 1 dia
  - **Respons√°vel:** Backend Dev

- [ ] **T1.1.3:** Setup do Sistema de Filas
  - Configurar RabbitMQ com exchanges e queues
  - Criar filas: `search-queue`, `enrichment-queue`, `validation-queue`
  - Management UI para monitoramento das filas
  - **Estimativa:** 2 dias
  - **Respons√°vel:** DevOps

#### T1.2: Configura√ß√£o do Web Scraping
- [ ] **T1.2.1:** Setup Puppeteer Otimizado
  - Configura√ß√£o de browser headless
  - Pool de browsers reutiliz√°veis
  - Tratamento de memory leaks
  - **Estimativa:** 2 dias
  - **Respons√°vel:** Scraping Specialist

- [ ] **T1.2.2:** Sistema de Proxies
  - Integra√ß√£o com provedor de proxies
  - Rota√ß√£o autom√°tica de IPs
  - Health check dos proxies
  - **Estimativa:** 2 dias
  - **Respons√°vel:** Scraping Specialist

- [ ] **T1.2.3:** Anti-Detection System
  - Rota√ß√£o de User-Agents
  - Randomiza√ß√£o de delays
  - Simula√ß√£o de comportamento humano
  - **Estimativa:** 3 dias
  - **Respons√°vel:** Scraping Specialist

### FASE 2: Core Scraping Engine (Sprint 2 - 3 semanas)

#### T2.1: Google Maps Search Engine
- [ ] **T2.1.1:** M√≥dulo de Busca Base
  - Implementar busca por termo + localiza√ß√£o
  - Navega√ß√£o entre p√°ginas de resultados
  - Extra√ß√£o de URLs dos estabelecimentos
  - **Estimativa:** 3 dias
  - **Respons√°vel:** Backend Dev

- [ ] **T2.1.2:** Extra√ß√£o de Dados Empresariais
  - Scraping de nome, endere√ßo, telefone
  - Captura de hor√°rio de funcionamento
  - Extra√ß√£o de fotos e avalia√ß√µes
  - Coleta de categoria/segmento
  - **Estimativa:** 4 dias
  - **Respons√°vel:** Scraping Specialist

- [ ] **T2.1.3:** Sistema de Parsing Robusto
  - Parser para diferentes layouts do GMaps
  - Fallbacks para mudan√ßas na interface
  - Tratamento de dados incompletos
  - **Estimativa:** 3 dias
  - **Respons√°vel:** Scraping Specialist

#### T2.2: Gest√£o de Sess√µes e Rate Limiting
- [ ] **T2.2.1:** Rate Limiting Inteligente
  - Controle de requests por minuto/hora
  - Backoff exponencial em caso de bloqueio
  - Monitoramento de CAPTCHAs
  - **Estimativa:** 2 dias
  - **Respons√°vel:** Backend Dev

- [ ] **T2.2.2:** Gest√£o de Sess√µes
  - Persist√™ncia de cookies entre requests
  - Simula√ß√£o de sess√µes longas
  - Logout/login autom√°tico quando necess√°rio
  - **Estimativa:** 2 dias
  - **Respons√°vel:** Scraping Specialist

### FASE 3: Data Enrichment (Sprint 3 - 2 semanas)

#### T3.1: Integra√ß√£o com APIs Externas
- [ ] **T3.1.1:** Integra√ß√£o ReceitaWS/Serasa
  - Busca de CNPJ por nome fantasia
  - Valida√ß√£o de dados empresariais
  - Enriquecimento com porte/setor
  - **Estimativa:** 3 dias
  - **Respons√°vel:** Backend Dev

- [ ] **T3.1.2:** Valida√ß√£o de Contatos
  - Verifica√ß√£o de n√∫meros de telefone
  - Valida√ß√£o de emails (se dispon√≠veis)
  - Normaliza√ß√£o de endere√ßos
  - **Estimativa:** 2 dias
  - **Respons√°vel:** Backend Dev

- [ ] **T3.1.3:** Sistema de Scoring
  - Algoritmo de qualidade dos dados
  - Classifica√ß√£o por confiabilidade
  - Prioriza√ß√£o de leads premium
  - **Estimativa:** 3 dias
  - **Respons√°vel:** Data Scientist

#### T3.2: Sistema de Categoriza√ß√£o
- [ ] **T3.2.1:** Classifica√ß√£o Autom√°tica
  - ML para categoriza√ß√£o de empresas
  - Mapeamento de segmentos
  - Tags autom√°ticas por nicho
  - **Estimativa:** 4 dias
  - **Respons√°vel:** Data Scientist

### FASE 4: Scheduler e Monitoring (Sprint 4 - 2 semanas)

#### T4.1: Sistema de Agendamento
- [ ] **T4.1.1:** Search Scheduler
  - Agendamento de buscas por regi√£o
  - Evitar sobreposi√ß√£o de esfor√ßos
  - Prioriza√ß√£o por demanda
  - **Estimativa:** 3 dias
  - **Respons√°vel:** Backend Dev

- [ ] **T4.1.2:** Job Management com RabbitMQ
  - Retry autom√°tico de jobs falhados via dead letter exchanges
  - Dead letter queue para an√°lise de falhas
  - TTL autom√°tico para cleanup de jobs antigos
  - **Estimativa:** 2 dias
  - **Respons√°vel:** Backend Dev

#### T4.2: Monitoring e Alertas
- [ ] **T4.2.1:** Dashboard de Monitoramento RabbitMQ
  - M√©tricas de filas em tempo real via Management UI
  - Alertas por Slack/Discord baseados em queue depth
  - Logs estruturados com correlation IDs
  - **Estimativa:** 3 dias
  - **Respons√°vel:** DevOps

- [ ] **T4.2.2:** Health Checks
  - Verifica√ß√£o de workers ativos via heartbeat
  - Status de conex√µes RabbitMQ
  - Status de proxies e browsers
  - Alertas de performance
  - **Estimativa:** 2 dias
  - **Respons√°vel:** DevOps

### FASE 5: Otimiza√ß√£o e Produ√ß√£o (Sprint 5 - 2 semanas)

#### T5.1: Performance Optimization
- [ ] **T5.1.1:** Otimiza√ß√£o de Queries
  - Indexes otimizados no PostgreSQL
  - Connection pooling
  - Cache de resultados frequentes
  - **Estimativa:** 2 dias
  - **Respons√°vel:** DBA

- [ ] **T5.1.2:** Scaling Horizontal
  - Auto-scaling de workers
  - Load balancing
  - Sharding de dados se necess√°rio
  - **Estimativa:** 3 dias
  - **Respons√°vel:** DevOps

#### T5.2: Deployment e CI/CD
- [ ] **T5.2.1:** Pipeline de Deploy
  - GitHub Actions para CI/CD
  - Testes automatizados
  - Deploy em staging/produ√ß√£o
  - **Estimativa:** 2 dias
  - **Respons√°vel:** DevOps

- [ ] **T5.2.2:** Backup e Recovery
  - Backup autom√°tico da base
  - Procedures de recovery
  - Testes de disaster recovery
  - **Estimativa:** 3 dias
  - **Respons√°vel:** DevOps

---

## üìä Schemas de Banco de Dados

**IMPORTANTE:** As tabelas abaixo ser√£o adicionadas ao schema existente em `/database/schema.sql` para integrar o sistema de coleta com o CRM atual.

### Integra√ß√£o com Schema Existente
O sistema de worker utilizar√° as seguintes tabelas j√° existentes:
- `organizations` - Para multi-tenancy
- `users` - Para auditoria de opera√ß√µes  
- `lead_segments` - Para categoriza√ß√£o autom√°tica
- `leads` - Destino final dos leads processados

### Novas Tabelas para o Worker

### Tabela: searches
```sql
CREATE TABLE searches (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  query VARCHAR(255) NOT NULL,
  location VARCHAR(255) NOT NULL,
  segment_id UUID REFERENCES lead_segments(id),
  status VARCHAR(20) DEFAULT 'pending' CHECK (status IN ('pending', 'running', 'completed', 'failed')),
  started_at TIMESTAMPTZ,
  completed_at TIMESTAMPTZ,
  results_count INTEGER DEFAULT 0,
  error_message TEXT,
  worker_id VARCHAR(100),
  created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
);
```

### Tabela: raw_leads
```sql
CREATE TABLE raw_leads (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  search_id UUID NOT NULL REFERENCES searches(id) ON DELETE CASCADE,
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  
  -- Dados coletados do Google Maps
  name VARCHAR(255) NOT NULL,
  address TEXT,
  phone VARCHAR(20),
  website VARCHAR(500),
  rating DECIMAL(2,1),
  reviews_count INTEGER,
  category VARCHAR(100),
  business_hours JSONB,
  photos JSONB,
  google_place_id VARCHAR(255) UNIQUE,
  google_url TEXT,
  
  -- Status do processamento
  processing_status VARCHAR(50) DEFAULT 'raw' CHECK (processing_status IN ('raw', 'enriching', 'enriched', 'validated', 'imported', 'failed')),
  quality_score INTEGER DEFAULT 0 CHECK (quality_score >= 0 AND quality_score <= 100),
  
  -- Timestamps
  created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
) PARTITION BY HASH (organization_id);

-- Criar parti√ß√µes para raw_leads (mesma estrat√©gia do leads)
DO $$
BEGIN
    FOR i IN 0..15 LOOP
        EXECUTE format('CREATE TABLE raw_leads_part_%s PARTITION OF raw_leads FOR VALUES WITH (modulus 16, remainder %s)', i, i);
    END LOOP;
END $$;
```

### Tabela: enriched_data
```sql
CREATE TABLE enriched_data (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  raw_lead_id UUID NOT NULL REFERENCES raw_leads(id) ON DELETE CASCADE,
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  
  -- Dados enriquecidos via APIs externas
  cnpj VARCHAR(18),
  company_size VARCHAR(50),
  sector VARCHAR(100),
  annual_revenue BIGINT,
  employees_count INTEGER,
  foundation_date DATE,
  legal_nature VARCHAR(100),
  verified_phone VARCHAR(20),
  verified_email VARCHAR(255),
  social_networks JSONB,
  technologies JSONB,
  
  -- Metadata do enriquecimento
  enrichment_source VARCHAR(100), -- 'receita_ws', 'serasa', 'google_places', etc.
  enrichment_confidence DECIMAL(3,2) DEFAULT 0.50,
  enrichment_cost DECIMAL(10,4) DEFAULT 0.00,
  
  created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
);
```

### Tabela: worker_logs
```sql
CREATE TABLE worker_logs (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  worker_id VARCHAR(100) NOT NULL,
  search_id UUID REFERENCES searches(id),
  level VARCHAR(10) NOT NULL CHECK (level IN ('debug', 'info', 'warn', 'error')),
  message TEXT NOT NULL,
  metadata JSONB,
  created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
);
```

### Processo de Integra√ß√£o dos Dados
1. **Coleta (raw_leads)** ‚Üí 2. **Enriquecimento (enriched_data)** ‚Üí 3. **Valida√ß√£o** ‚Üí 4. **Importa√ß√£o (leads)**

A integra√ß√£o final transfere dados de `raw_leads` + `enriched_data` para a tabela `leads` existente, mantendo a compatibilidade com o CRM.

---

## üîß Configura√ß√µes do Sistema

### Puppeteer Configuration
```typescript
const browserConfig = {
  headless: true,
  args: [
    '--no-sandbox',
    '--disable-setuid-sandbox',
    '--disable-dev-shm-usage',
    '--disable-accelerated-2d-canvas',
    '--no-first-run',
    '--no-zygote',
    '--disable-gpu',
    '--window-size=1920,1080'
  ],
  defaultViewport: {
    width: 1920,
    height: 1080
  }
};
```

### Rate Limiting Rules
```yaml
rate_limits:
  requests_per_minute: 30
  requests_per_hour: 1000
  concurrent_browsers: 5
  delay_between_requests: 2-5s
  backoff_on_error: exponential
  max_retries: 3
```

### RabbitMQ Configuration
```yaml
rabbitmq:
  connection:
    host: "localhost"
    port: 5672
    username: "leadsrapido"
    password: "secure_password"
    vhost: "/leadsrapido"
  
  exchanges:
    search_exchange:
      type: "direct"
      durable: true
    enrichment_exchange:
      type: "topic"
      durable: true
    dlx_exchange:
      type: "direct"
      durable: true
  
  queues:
    search_queue:
      durable: true
      exclusive: false
      auto_delete: false
      arguments:
        x-message-ttl: 3600000  # 1 hour
        x-dead-letter-exchange: "dlx_exchange"
    
    enrichment_queue:
      durable: true
      exclusive: false
      auto_delete: false
      arguments:
        x-message-ttl: 1800000  # 30 minutes
        x-dead-letter-exchange: "dlx_exchange"
    
    validation_queue:
      durable: true
      exclusive: false
      auto_delete: false
      arguments:
        x-message-ttl: 900000   # 15 minutes
        x-dead-letter-exchange: "dlx_exchange"
    
    dead_letter_queue:
      durable: true
      exclusive: false
      auto_delete: false

  prefetch_count: 1  # Process one message at a time per worker
  ack_timeout: 30000 # 30 seconds
```

### RabbitMQ Setup Commands
```bash
# Install RabbitMQ (Ubuntu/Debian)
sudo apt-get install rabbitmq-server

# Enable management plugin
sudo rabbitmq-plugins enable rabbitmq_management

# Create virtual host
sudo rabbitmqctl add_vhost /leadsrapido

# Create user
sudo rabbitmqctl add_user leadsrapido secure_password
sudo rabbitmqctl set_permissions -p /leadsrapido leadsrapido ".*" ".*" ".*"
sudo rabbitmqctl set_user_tags leadsrapido administrator

# Docker alternative
docker run -d --name rabbitmq \
  -p 5672:5672 \
  -p 15672:15672 \
  -e RABBITMQ_DEFAULT_USER=leadsrapido \
  -e RABBITMQ_DEFAULT_PASS=secure_password \
  -e RABBITMQ_DEFAULT_VHOST=/leadsrapido \
  rabbitmq:3-management
```

### Search Patterns
```typescript
const searchTemplates = [
  "{segment} em {city}",
  "{segment} {city} {state}",
  "{segment} pr√≥ximo a {neighborhood}",
  "melhor {segment} {city}",
  "{segment} {city} telefone"
];
```

---

## üìà M√©tricas e KPIs

### Performance Metrics
- **Leads coletados por hora:** Target > 500/h
- **Taxa de sucesso:** > 95%
- **Tempo m√©dio por busca:** < 30s
- **Qualidade dos dados:** Score m√©dio > 8/10
- **Duplicatas:** < 5%

### Business Metrics
- **Cobertura geogr√°fica:** 100% capitais, 80% interior
- **Segmentos ativos:** > 50 categorias
- **Leads √∫nicos mensais:** > 50.000
- **Taxa de convers√£o:** > 2%
- **ROI por lead:** R$ 2,50 custo vs R$ 25 valor

### Technical Metrics
- **Uptime do sistema:** > 99.5%
- **Memory usage:** < 2GB por worker
- **CPU usage:** < 80% m√©dia
- **Database response time:** < 100ms
- **Queue processing time:** < 5min m√©dia

---

## üö® Riscos e Mitiga√ß√µes

### Riscos T√©cnicos
1. **Bloqueio pelo Google Maps**
   - **Mitiga√ß√£o:** Proxy rotation, delays aleat√≥rios, comportamento humano simulado
   
2. **Mudan√ßas na Interface**
   - **Mitiga√ß√£o:** Multiple selectors, fallback strategies, automated tests
   
3. **Rate Limiting Agressivo**
   - **Mitiga√ß√£o:** Distributed workers, smart scheduling, queue management

### Riscos de Neg√≥cio
1. **Qualidade dos Dados**
   - **Mitiga√ß√£o:** Multiple validation layers, human review samples, feedback loops
   
2. **Compliance/Legal**
   - **Mitiga√ß√£o:** LGPD compliance, public data only, opt-out mechanisms

### Riscos Operacionais
1. **Escalabilidade**
   - **Mitiga√ß√£o:** Horizontal scaling, load testing, performance monitoring
   
2. **Manuten√ß√£o**
   - **Mitiga√ß√£o:** Automated updates, monitoring alerts, documentation

---

## üìÖ Timeline de Entrega

| Fase | Dura√ß√£o | In√≠cio | T√©rmino | Entreg√°veis |
|------|---------|--------|---------|-------------|
| **FASE 1** | 2 semanas | 20/08 | 02/09 | Infraestrutura base funcional |
| **FASE 2** | 3 semanas | 03/09 | 23/09 | Scraping engine completo |
| **FASE 3** | 2 semanas | 24/09 | 07/10 | Data enrichment ativo |
| **FASE 4** | 2 semanas | 08/10 | 21/10 | Monitoring e scheduling |
| **FASE 5** | 2 semanas | 22/10 | 04/11 | Sistema em produ√ß√£o |

**Data de Go-Live:** 04/11/2025

---

## üîê Aspectos de Seguran√ßa

### Data Protection
- Criptografia de dados sens√≠veis
- Anonimiza√ß√£o de informa√ß√µes pessoais
- Logs audit√°veis de acesso
- Backup criptografado

### Access Control
- Autentica√ß√£o multifator para admin
- Roles e permiss√µes granulares
- API keys com escopo limitado
- Network segmentation

### Compliance
- LGPD compliance total
- Terms of service claros
- Opt-out mechanisms
- Data retention policies

---

## üìö Documenta√ß√£o Adicional

### Para Desenvolvimento
- [ ] API Documentation (Swagger)
- [ ] Database Schema Documentation
- [ ] Development Setup Guide
- [ ] Testing Strategy Document

### Para Opera√ß√£o
- [ ] Deployment Guide
- [ ] Monitoring Playbook
- [ ] Incident Response Procedures
- [ ] Performance Tuning Guide

### Para Neg√≥cio
- [ ] User Manual
- [ ] Data Quality Guidelines
- [ ] Compliance Documentation
- [ ] ROI Analysis Report

---

**Criado por:** Equipe LeadsRapido  
**√öltima atualiza√ß√£o:** 20/08/2025  
**Vers√£o:** 1.0  
**Status:** Em planejamento
